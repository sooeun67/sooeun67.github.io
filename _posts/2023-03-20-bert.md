---
layout: single
title:  "[딥러닝/AI] 내가 이해하려고 만든 BERT 정리본"
categories:
  - data science
tags:
  - AI
---



## What's BERT?

BERT는 두 단계로 이루어진다

👆 사전 학습 (Pre-training): 영문 위키피디아와 BooksCorpus의 데이터를 사용하여 두 가지 학습을 하는데, 하나는 단어를 무작위로 가려놓고 원래 단어를 맞추는 학습 (Masked LM), 다른 하나는 문장의 순서를 예측하는 것을 학습합니다 (Next Sentence Prediction, NSP).

✌️ 미세 조정 (Fine-tuning): 사전학습 시 사용된 파라미터를 모두 조정하여 모델이 어느 한 태스크에 국한되지 않고 범용적으로 쓰일 수 있도록 합니다.



## KeyBERT
### <U>키워드 추출을 위한 BERT 기반의 Package</U>
- 원리: BERT 를 이용해 document embeddings 추출하고, word embeddings 추출(N-gram words/phrases 위한)한다.
그러고 나서, cosine similarity 로 문서와 가장 유사한 words/phrases 를 찾는다 => 가장 유사한 단어들이 전체 문서를 가장 잘 설명하는 **"키워드"** 가 된다
- [Official Github](https://github.com/MaartenGr/KeyBERT)
- Features: 
  1) keyword 로 할지 keyphrase로 할지 argument 줄 수 있음
  2) any BERT model 가능 => fixed-size vector 로 변환되게 설계되어 있
  - 


## DistilBERT
### <U>경량화 버전의 BERT </U>
- 40% smaller, 60% faster version of BERT
- 

## RoBERTa
### <U>trained larger dataset </U>
- batch size 증가시켜 학습시킴
- NSP 는 없애고, MLM task 만 학습에 사용 -- masking task 에서 정적 마스킹이 아니라 동적 마스킹을 적용
- > 동적 마스킹을 적용하면, 